{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract electricity prices and volume from VENRON data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we use `Fonduer` to extract relations from the `VENRON` dataset.  \n",
    "This code is a modified version of their original hardware [tutorial](https://github.com/HazyResearch/fonduer-tutorials/tree/master/hardware).  \n",
    "The `Fonduer` pipeline (as outlined in the [paper](https://arxiv.org/abs/1703.05028)), and the iterative KBC process:\n",
    "\n",
    "1. KBC Initialization\n",
    "2. Candidate Generation and Multimodal Featurization\n",
    "3. Probabilistic Relation Classification\n",
    "4. Error Analysis and Iterative KBC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we import the relevant libraries and connect to the local database.  \n",
    "Follow the README instructions to setup the connection to the postgres DB correctly.\n",
    "\n",
    "If the database has existing candidates with generated features, the will not be overriden.  \n",
    "To re-run the entire pipeline including initialization drop the database first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dropdb -h postgres -h postgres -h postgres -h postgres -h postgres -h postgres --if-exists elec_price_vol\n",
    "! createdb -h postgres -h postgres -h postgres -h postgres -h postgres -h postgres elec_price_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PARALLEL = 8 # 4  # assuming a quad-core machine\n",
    "ATTRIBUTE = \"elec_price_vol\"\n",
    "DB_USERNAME = 'user'\n",
    "DB_PASSWORD = 'venron'\n",
    "conn_string = f'postgresql://{DB_USERNAME}:{DB_PASSWORD}@postgres:5432/{ATTRIBUTE}'\n",
    "    \n",
    "docs_path = 'data/gold/html/'\n",
    "pdf_path = 'data/gold/pdf/'\n",
    "gold_file = 'data/electricity_gold.csv'\n",
    "max_docs = 50 # 114\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "We first initialize a `Meta` object, which manages the connection to the database automatically, and enables us to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fonduer import Meta, init_logging\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "init_logging(log_dir=\"logs\")\n",
    "\n",
    "session = Meta.init(conn_string).Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
    "from fonduer.parser.models import Document, Sentence\n",
    "from fonduer.parser import Parser\n",
    "\n",
    "has_documents = session.query(Document).count() > 0\n",
    "\n",
    "corpus_parser = Parser(session, structural=True, lingual=True, visual=True, pdf_path=pdf_path)\n",
    "\n",
    "if (not has_documents): \n",
    "    doc_preprocessor = HTMLDocPreprocessor(docs_path, max_docs=max_docs)\n",
    "    %time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)\n",
    "    \n",
    "print(f\"Documents: {session.query(Document).count()}\")\n",
    "print(f\"Sentences: {session.query(Sentence).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dividing the Corpus into Test and Train\n",
    "\n",
    "We'll split the documents 80/10/10 into train/dev/test splits. Note that here we do this in a non-random order to preserve the consistency and we reference the splits by 0/1/2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs][0:5])\n",
    "print(f\"Number of documents split: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Mention Extraction, Candidate Extraction Multimodal Featurization\n",
    "\n",
    "Given the unified data model from Phase 1, `Fonduer` extracts relation\n",
    "candidates based on user-provided **matchers** and **throttlers**. Then,\n",
    "`Fonduer` leverages the multimodality information captured in the unified data\n",
    "model to provide multimodal features for each candidate.\n",
    "\n",
    "## 2.1 Mention Extraction & Candidate Generation\n",
    "\n",
    "1. Define mention classes\n",
    "2. Use matcher functions to define the format of potential mentions\n",
    "3. Define Mentionspaces (Ngrams)\n",
    "4. Run Mention extraction (all possible ngrams in the document, API [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#fonduer.candidates.MentionExtractor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import mention_subclass\n",
    "from fonduer.candidates.matchers import RegexMatchSpan, DictionaryMatch, LambdaFunctionMatcher, Intersect, Union\n",
    "from fonduer.candidates import MentionNgrams\n",
    "from fonduer.candidates import MentionExtractor \n",
    "from fonduer.candidates.models import Mention\n",
    "\n",
    "hasMentions = session.query(Mention).count() > 0\n",
    "\n",
    "# 1.) Mention subclasses\n",
    "Station = mention_subclass(\"Station\")\n",
    "Price = mention_subclass(\"Price\")\n",
    "\n",
    "### Dictionary of known stations ###\n",
    "stations = [\n",
    "    [\n",
    "        \"cob\",\n",
    "        \"california-oregon border\",\n",
    "        \"california-oregon border (cob)\",\n",
    "    ],\n",
    "    [\n",
    "        \"palo verde\",\n",
    "        \"palo\",\n",
    "    ],\n",
    "    [\n",
    "        \"mid columbia\",\n",
    "        \"mid-columbia\",\n",
    "        \"midc\",\n",
    "    ],\n",
    "    [\n",
    "        \"mead\",\n",
    "        \"meadmktplace\",\n",
    "        \"mead/marketplace\",\n",
    "    ],\n",
    "    [\n",
    "        \"np-15\",\n",
    "        \"np 15\",\n",
    "        \"california northern zone\",\n",
    "        \"california northern zone (np-15)\",\n",
    "    ],\n",
    "    [\n",
    "        \"sp-15\",\n",
    "        \"sp 15\",\n",
    "        \"california southern zone\",\n",
    "        \"california southern zone (sp-15)\",\n",
    "    ],\n",
    "    [\n",
    "        \"pjm â€“ western hub\",\n",
    "        \"pjm\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "stations_mapping_dict = { k:station_list for station_list in stations for k in station_list }\n",
    "stations_list = [s for station_list in stations for s in station_list]\n",
    "\n",
    "if (not hasMentions):\n",
    "\n",
    "    # 2.) Matcher functions\n",
    "    station_matcher = DictionaryMatch(d=stations_list)\n",
    "    price_matcher = RegexMatchSpan(rgx=r\"\\d{1,4}(\\.\\d{1,5})\", longest_match_only=False)\n",
    "\n",
    "    # 3.) Mention spaces (Ngrams)\n",
    "    station_ngrams = MentionNgrams(n_max=4)\n",
    "    price_ngrams = MentionNgrams(n_max=1)\n",
    "\n",
    "\n",
    "    # 4.) Mention extraction\n",
    "    mention_extractor = MentionExtractor(\n",
    "        session, [Station, Price], [station_ngrams, price_ngrams], [station_matcher, price_matcher]\n",
    "    )\n",
    "    docs = session.query(Document).order_by(Document.name).all()\n",
    "    mention_extractor.apply(docs, parallelism=PARALLEL)\n",
    "\n",
    "    \n",
    "print(f\"Total Mentions: {session.query(Mention).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Candidate Extraction\n",
    "\n",
    "1. Define Candidate Class\n",
    "2. Define trottlers to reduce the number of possible candidates\n",
    "3. Extract candidates (View the API for the CandidateExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#fonduer.candidates.MentionExtractor).)\n",
    "\n",
    "In the last part we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train/dev/test as splits 0/1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import *\n",
    "import re\n",
    "from fonduer.candidates import CandidateExtractor\n",
    "from fonduer.candidates.models import candidate_subclass\n",
    "from fonduer.utils.visualizer import Visualizer\n",
    "\n",
    "\n",
    "# 1.) Define Candidate class\n",
    "StationPrice = candidate_subclass(\"StationPrice\", [Station, Price])\n",
    "\n",
    "has_candidates = session.query(StationPrice).filter(StationPrice.split == 0).count() > 0\n",
    "\n",
    "# 2.) DefineThrottlers\n",
    "def any_filter(c):\n",
    "    (station, price) = c\n",
    "    if 'volume' in get_aligned_ngrams(price, lower=True):\n",
    "        return False\n",
    "    if 'date' in get_aligned_ngrams(price, lower=True):\n",
    "        return False \n",
    "    if 'non' in get_aligned_ngrams(price, lower=True):\n",
    "        return False \n",
    "    return True\n",
    "\n",
    "any_throttler = any_filter\n",
    "\n",
    "# 3.) Candidate extraction\n",
    "candidate_extractor = CandidateExtractor(session, [StationPrice], throttlers=[any_throttler])\n",
    "\n",
    "for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "    if (not has_candidates):\n",
    "        candidate_extractor.apply(docs, split=i, parallelism=PARALLEL)\n",
    "    print(f\"Number of Candidates in split={i}: {session.query(StationPrice).filter(StationPrice.split == i).count()}\")\n",
    "\n",
    "train_cands = candidate_extractor.get_candidates(split = 0)\n",
    "dev_cands = candidate_extractor.get_candidates(split = 1)\n",
    "test_cands = candidate_extractor.get_candidates(split = 2)\n",
    "\n",
    "\n",
    "# 4.) Visualize some candidate for error analysis\n",
    "pprint(train_cands[0][2003])\n",
    "vis = Visualizer(pdf_path)\n",
    "\n",
    "# Display a candidate\n",
    "vis.display_candidates([train_cands[0][2003]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Multimodal Featurization\n",
    "Unlike dealing with plain unstructured text, `Fonduer` deals with richly formatted data, and consequently featurizes each candidate with a baseline library of multimodal features. \n",
    "\n",
    "### Featurize with `Fonduer`'s optimized Postgres Featurizer\n",
    "We now annotate the candidates in our training, dev, and test sets with features. The `Featurizer` provided by `Fonduer` allows this to be done in parallel to improve performance.\n",
    "\n",
    "View the API provided by the `Featurizer` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/features.html#fonduer.features.Featurizer).\n",
    "\n",
    "At the end of this phase, `Fonduer` has generated the set of candidates and the feature matrix. Note that Phase 1 and 2 are relatively static and typically are only executed once during the KBC process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fonduer.features import Featurizer\n",
    "from fonduer.features.models import Feature\n",
    "\n",
    "featurizer = Featurizer(session, [StationPrice])\n",
    "\n",
    "has_features = session.query(Feature).count() > 0\n",
    "\n",
    "if (not has_features):\n",
    "    # Training set\n",
    "    %time featurizer.apply(split=0, train=True, parallelism=PARALLEL)\n",
    "    %time F_train = featurizer.get_feature_matrices(train_cands)\n",
    "    print(F_train[0].shape)\n",
    "\n",
    "    # Dev set\n",
    "    %time featurizer.apply(split=1, parallelism=PARALLEL)\n",
    "    %time F_dev = featurizer.get_feature_matrices(dev_cands)\n",
    "    print(F_dev[0].shape)\n",
    "\n",
    "    # Test set\n",
    "    %time featurizer.apply(split=2, parallelism=PARALLEL)\n",
    "    %time F_test = featurizer.get_feature_matrices(test_cands)\n",
    "    print(F_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 3: Probabilistic Relation Classification\n",
    "In this phase, `Fonduer` applies user-defined **labeling functions**, which express various heuristics, patterns, and [weak supervision](http://hazyresearch.github.io/snorkel/blog/weak_supervision.html) strategies to label our data, to each of the candidates to create a label matrix that is used by our data programming engine.\n",
    "\n",
    "1. Load Gold Data\n",
    "\n",
    "--- \n",
    "\n",
    "Iterate the following steps\n",
    "\n",
    "2. Create labeling functions\n",
    "3. Apply labeling functions and measure accuracy of each LF (based on gold data).\n",
    "4. Build a generative model by combining the labeling functions\n",
    "5. Iterate on labeling function based on the models score\n",
    "\n",
    "---\n",
    "\n",
    "6. Finally build a descriminative model and test on the test set\n",
    "\n",
    "### 3.1) Loading Gold LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fonduer.supervision.models import GoldLabel\n",
    "from electricity_utils import get_gold_func\n",
    "from fonduer.supervision import Labeler\n",
    "\n",
    "# 1.) Load the gold data\n",
    "gold = get_gold_func(gold_file, attribute=ATTRIBUTE, stations_mapping_dict=stations_mapping_dict)\n",
    "docs = corpus_parser.get_documents()\n",
    "labeler = Labeler(session, [StationPrice])\n",
    "%time labeler.apply(docs=docs, lfs=[[gold]], table=GoldLabel, train=True, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2) Creating Labeling Functions\n",
    "\n",
    "We have 3 states that we can return from a LF: `ABSTAIN`, `FALSE` or `TRUE`.\n",
    "\n",
    "A library of data model utilities\n",
    "which can be used to write labeling functions are outline in [Read the\n",
    "Docs](http://fonduer.readthedocs.io/en/stable/user/data_model_utils.html). \n",
    "\n",
    "### 3.3) Applying the Labeling Functions\n",
    "\n",
    "Next, we need to actually run the LFs over all of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database. Note that this will delete any existing `Labels` and `LabelKeys` for this candidate set.\n",
    "\n",
    "View the API provided by the `Labeler` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/supervision.html#fonduer.supervision.Labeler).\n",
    "\n",
    "We can also view statistics about the resulting label matrix.\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for.\n",
    "\n",
    "In addition, because we have already loaded the gold labels, we can view the emperical accuracy of these labeling functions when compared to our gold labels using the `analysis` module of [Snorkel](https://github.com/snorkel-team/snorkel)\n",
    "\n",
    "### 3.4) Build Generative Model\n",
    "\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other. To do so, we use [Snorkel](https://github.com/snorkel-team/snorkel)'s single-task label model.\n",
    "\n",
    "We then print out the marginal probabilities for each training candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models.span_mention import TemporarySpanMention\n",
    "from fonduer.candidates.models import Candidate, Mention\n",
    "from typing import Iterator, Tuple, Union\n",
    "from fonduer.utils.data_model_utils.tabular import _get_aligned_sentences\n",
    "from fonduer.utils.utils_table import min_col_diff, min_row_diff, is_axis_aligned\n",
    "from fonduer.utils.utils import tokens_to_ngrams\n",
    "from fonduer.utils.data_model_utils.utils import _to_span, _to_spans\n",
    "from itertools import chain\n",
    "\n",
    "def get_neighbor_cell_ngrams_own(\n",
    "    mention: Union[Candidate, Mention, TemporarySpanMention],\n",
    "    dist: int = 1,\n",
    "    directions: bool = False,\n",
    "    attrib: str = \"words\",\n",
    "    n_min: int = 1,\n",
    "    n_max: int = 1,\n",
    "    lower: bool = True,\n",
    "    absolute: bool = False,\n",
    ") -> Iterator[Union[str, Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Get the ngrams from all Cells that are within a given Cell distance in one\n",
    "    direction from the given Mention.\n",
    "\n",
    "    Note that if a candidate is passed in, all of its Mentions will be\n",
    "    searched. If `directions=True``, each ngram will be returned with a\n",
    "    direction in {'UP', 'DOWN', 'LEFT', 'RIGHT'}.\n",
    "\n",
    "    :param mention: The Mention whose neighbor Cells are being searched\n",
    "    :param dist: The Cell distance within which a neighbor Cell must be to be\n",
    "        considered\n",
    "    :param directions: A Boolean expressing whether or not to return the\n",
    "        direction of each ngram\n",
    "    :param attrib: The token attribute type (e.g. words, lemmas, poses)\n",
    "    :param n_min: The minimum n of the ngrams that should be returned\n",
    "    :param n_max: The maximum n of the ngrams that should be returned\n",
    "    :param lower: If True, all ngrams will be returned in lower case\n",
    "    :rtype: a *generator* of ngrams (or (ngram, direction) tuples if directions=True)\n",
    "    \"\"\"\n",
    "    # TODO: Fix this to be more efficient (optimize with SQL query)\n",
    "    spans = _to_spans(mention)\n",
    "    for span in spans:\n",
    "        for ngram in get_sentence_ngrams(\n",
    "            span, attrib=attrib, n_min=n_min, n_max=n_max, lower=lower\n",
    "        ):\n",
    "            yield ngram\n",
    "        if span.sentence.is_tabular():\n",
    "            root_cell = span.sentence.cell\n",
    "            for sentence in chain.from_iterable(\n",
    "                [\n",
    "                    _get_aligned_sentences(root_cell, \"row\"),\n",
    "                    _get_aligned_sentences(root_cell, \"col\"),\n",
    "                ]\n",
    "            ):\n",
    "                row_diff = min_row_diff(sentence, root_cell, absolute=absolute)\n",
    "                col_diff = min_col_diff(sentence, root_cell, absolute=absolute)\n",
    "                if (\n",
    "                    (row_diff or col_diff)\n",
    "                    and not (row_diff and col_diff)\n",
    "                    and abs(row_diff) + abs(col_diff) <= dist\n",
    "                ):\n",
    "                    if directions:\n",
    "                        direction = \"\"\n",
    "                        if col_diff == 0:\n",
    "                            if 0 < row_diff and row_diff <= dist:\n",
    "                                direction = \"UP\"\n",
    "                            elif 0 > row_diff and row_diff >= -dist:\n",
    "                                direction = \"DOWN\"\n",
    "                        elif row_diff == 0:\n",
    "                            if 0 < col_diff and col_diff <= dist:\n",
    "                                direction = \"RIGHT\"\n",
    "                            elif 0 > col_diff and col_diff >= -dist:\n",
    "                                direction = \"LEFT\"\n",
    "                        for ngram in tokens_to_ngrams(\n",
    "                            getattr(sentence, attrib),\n",
    "                            n_min=n_min,\n",
    "                            n_max=n_max,\n",
    "                            lower=lower,\n",
    "                        ):\n",
    "                            yield (ngram, direction)\n",
    "                    else:\n",
    "                        for ngram in tokens_to_ngrams(\n",
    "                            getattr(sentence, attrib),\n",
    "                            n_min=n_min,\n",
    "                            n_max=n_max,\n",
    "                            lower=lower,\n",
    "                        ):\n",
    "                            yield ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to check applied labelling functions\n",
    "from fonduer.supervision.models import Label\n",
    "from sqlalchemy import func\n",
    "\n",
    "def get_applied_lfs(): \n",
    "    labels = session.query(Label).all()\n",
    "    ls = [x for x in labels if len(x.values) > 0]\n",
    "    lfs = [lf for l in ls for lf in l.keys]\n",
    "    return set(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import *\n",
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "ABSTAIN = -1\n",
    "FALSE = 0\n",
    "TRUE = 1\n",
    "\n",
    "# Basic constraint for the price LFs to be true -> no wrong station (increase accuracy)\n",
    "def base(c):\n",
    "    return LF_station_non_meta_tag(c) != 0 and LF_other_station_table_new(c) != 0\n",
    "\n",
    "# 2.) Create labeling functions \n",
    "@labeling_function()\n",
    "def LF_price_head(c):\n",
    "    return TRUE if 'price' in get_aligned_ngrams(c.price) and base(c) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def LF_on_peak_head(c):\n",
    "    return TRUE if 'on peak' in get_aligned_ngrams(c.price, n_min=2, n_max=2)  and base(c) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def LF_off_peak_head(c):\n",
    "    return FALSE if 'off peak' in get_aligned_ngrams(c.price, n_min=2, n_max=2) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def LF_firm_head(c):\n",
    "    return TRUE if 'firm' in get_aligned_ngrams(c.price)and base(c) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def LF_non_firm_head(c):\n",
    "    return FALSE if 'non firm' in get_aligned_ngrams(c.price, n_min=2, n_max=2) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def LF_dollar_to_left(c):\n",
    "    return TRUE if '$' in get_left_ngrams(c.price, window=2) and base(c) else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def LF_price_range(c):\n",
    "    return TRUE if c.price > 0 and c.price < 1000 and base(c) else FALSE\n",
    "\n",
    "@labeling_function()\n",
    "def LF_other_station_table_new(c):\n",
    "    station_span = c.station.context.get_span().lower()\n",
    "    neighbour_cells = get_neighbor_cell_ngrams_own(c.price, dist=2, directions=True, n_max = 4, absolute = True)\n",
    "    up_cells = [x for x in neighbour_cells if len(x) > 1 and x[1] == 'UP' and x[0] in stations_list]\n",
    "    # No station name in upper cells\n",
    "    if (len(up_cells) == 0):\n",
    "        return ABSTAIN\n",
    "    # Check if the next upper aligned station-span corresponds to the candidate span (or equivalents)\n",
    "    for m in up_cells:\n",
    "        if (m[0] not in stations_mapping_dict[station_span]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "@labeling_function()\n",
    "def LF_station_non_meta_tag(c):\n",
    "    html_tags = get_ancestor_tag_names(c.station)\n",
    "    return FALSE if ('head' in html_tags and 'title' in html_tags) else ABSTAIN\n",
    "\n",
    "\n",
    "station_price_lfs = [\n",
    "    LF_price_head,\n",
    "    LF_on_peak_head,\n",
    "    LF_off_peak_head,\n",
    "    LF_firm_head,\n",
    "    LF_non_firm_head,\n",
    "    LF_dollar_to_left,\n",
    "    LF_price_range\n",
    "    LF_other_station_table,\n",
    "    LF_station_non_meta_tag,\n",
    "]\n",
    "\n",
    "# 3.) Apply the LFs on the training set\n",
    "labeler = Labeler(session, [StationPrice])\n",
    "%time labeler.apply(split=0, lfs=[station_price_lfs], train=True, clear=True, parallelism=PARALLEL)\n",
    "%time L_train = labeler.get_label_matrices(train_cands)\n",
    "\n",
    "# Check that LFs are all applied (avoid crash)\n",
    "applied_lfs = L_train[0].shape[1]\n",
    "has_non_applied = applied_lfs != len(station_price_lfs)\n",
    "print(f\"Labeling functions on train_cands not ABSTAIN: {applied_lfs} (/{len(station_price_lfs)})\")\n",
    "\n",
    "if (has_non_applied):\n",
    "    applied_lfs = get_applied_lfs()\n",
    "    non_applied_lfs = [l.name for l in station_price_lfs if l.name not in applied_lfs]\n",
    "    print(f\"Labling functions {non_applied_lfs} are not applied.\")\n",
    "    station_price_lfs = [l for l in station_price_lfs if l.name in applied_lfs]\n",
    "\n",
    "# 4.) Evaluate their accuracy\n",
    "L_gold_train = labeler.get_gold_labels(train_cands, annotator='gold')\n",
    "# Sort LFs for LFAnalysis because LFAnalysis does not sort LFs,\n",
    "# while columns of L_train are sorted alphabetically already.\n",
    "sorted_lfs = sorted(station_price_lfs, key=lambda lf: lf.name)\n",
    "LFAnalysis(L=L_train[0], lfs=sorted_lfs).lf_summary(Y=L_gold_train[0].reshape(-1))\n",
    "\n",
    "# 5.) Build generative model\n",
    "gen_model = LabelModel(cardinality=2)\n",
    "%time gen_model.fit(L_train[0], n_epochs=500, log_freq=100)\n",
    "\n",
    "train_marginals = gen_model.predict_proba(L_train[0])\n",
    "plt.hist(train_marginals[:, TRUE], bins=20)\n",
    "plt.show()\n",
    "\n",
    "# Apply on dev-set\n",
    "labeler.apply(split=1, lfs=[station_price_lfs], clear=True, parallelism=PARALLEL)\n",
    "%time L_dev = labeler.get_label_matrices(dev_cands)\n",
    "\n",
    "L_gold_dev = labeler.get_gold_labels(dev_cands, annotator='gold')\n",
    "LFAnalysis(L=L_dev[0], lfs=sorted_lfs).lf_summary(Y=L_gold_dev[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for analysis\n",
    "labels = session.query(Label).all()\n",
    "gold_labels = session.query(GoldLabel).all()\n",
    "gold_labels_map = { gold_label.candidate_id: gold_label for gold_label in gold_labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import Candidate\n",
    "\n",
    "DB_FALSE = FALSE +1\n",
    "DB_ABSTAIN = ABSTAIN +1\n",
    "DB_TRUE = TRUE +1\n",
    "\n",
    "def get_incorrect_instances(lf):\n",
    "    def is_wrong_label(label):\n",
    "        if (lf.name not in label.keys):\n",
    "            return False # Abstain\n",
    "        assigned_label = label.values[label.keys.index(lf.name)]\n",
    "        gold_label = gold_labels_map[label.candidate_id] # [x for x in gold_labels if x.candidate_id == label.candidate_id][0]\n",
    "        return gold_label.values[0] != assigned_label\n",
    "    return [x.candidate for x in labels if is_wrong_label(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = station_price_lfs[4]\n",
    "wrong_cands = get_incorrect_instances(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f\"Labeling Function: {lf.name} has wrongly labelled the candidate(1/{len(wrong_cands)}):\")\n",
    "if (len(wrong_cands) > 0):\n",
    "    wrong_cand = wrong_cands[0]\n",
    "    pprint(wrong_cand)\n",
    "    pprint('LF is True' if lf(wrong_cand) == 1 else 'LF is False')\n",
    "    vis = Visualizer(pdf_path)\n",
    "\n",
    "    # Display a candidate\n",
    "    vis.display_candidates([wrong_cand])\n",
    "else:\n",
    "    print(\"There are no wrong candidates for this labeling function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,x.document.name) for i,x in enumerate(wrong_cands)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = cand[0]\n",
    "# o = overlap(\n",
    "#         dict_without_station(c.station.context.get_span()), \n",
    "#         list(get_aligned_ngrams(c.price))\n",
    "#     )\n",
    "# station_grams = [stationNgram.lower() for stationNgram in dict_without_station(c.station.context.get_span())]\n",
    "# ngrams = list(get_aligned_ngrams(c.price, n_min=1, n_max=4, lower=True))\n",
    "# ngrams_filtered = [g for g in ngrams if g in station_grams]\n",
    "# # overlap(station_grams, ngrams)\n",
    "# root_sentence = _to_span(c.price).sentence\n",
    "# cells = _get_table_cells(root_sentence.table).items()\n",
    "# aligned_sentences = [\n",
    "#         sentence\n",
    "#         for (cell, sentences) in cells\n",
    "#         if is_axis_aligned(root_sentence, cell)\n",
    "#         for sentence in sentences\n",
    "#         if sentence != root_sentence\n",
    "#     ]\n",
    "# aligned_ngrams = [x for x in aligned_sentences if x.text.lower() in station_grams]\n",
    "# root_sentence.words # col 2, row 22\n",
    "# tokens_to_ngrams(\n",
    "#                         getattr(sentence, attrib), n_min=n_min, n_max=n_max, lower=lower\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Discriminative Model \n",
    "\n",
    "Fonduer uses the machine learning framework [Emmental](https://github.com/SenWu/emmental) to support all model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emmental\n",
    "\n",
    "# Setup training config\n",
    "config = {\n",
    "    \"meta_config\": {\"verbose\": True},\n",
    "    \"model_config\": {\"model_path\": None, \"device\": 0, \"dataparallel\": False},\n",
    "    \"learner_config\": {\n",
    "        \"n_epochs\": 50,\n",
    "        \"optimizer_config\": {\"lr\": 0.001, \"l2\": 0.0},\n",
    "        \"task_scheduler\": \"round_robin\",\n",
    "    },\n",
    "    \"logging_config\": {\n",
    "        \"evaluation_freq\": 1,\n",
    "        \"counter_unit\": \"epoch\",\n",
    "        \"checkpointing\": False,\n",
    "        \"checkpointer_config\": {\n",
    "            \"checkpoint_metric\": {f\"{ATTRIBUTE}/{ATTRIBUTE}/train/loss\": \"min\"},\n",
    "            \"checkpoint_freq\": 1,\n",
    "            \"checkpoint_runway\": 2,\n",
    "            \"clear_intermediate_checkpoints\": True,\n",
    "            \"clear_all_checkpoints\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "emmental.init(Meta.log_path)\n",
    "emmental.Meta.update_config(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect word counter from training data\n",
    "from fonduer.learning.utils import collect_word_counter\n",
    "\n",
    "word_counter = collect_word_counter(train_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word embedding module for LSTM model\n",
    "# (in Logistic Regression, we generate it since Fonduer dataset requires word2id dict)\n",
    "from emmental.modules.embedding_module import EmbeddingModule\n",
    "\n",
    "arity = 2\n",
    "\n",
    "# Geneate special tokens\n",
    "specials = []\n",
    "for i in range(arity):\n",
    "    specials += [f\"~~[[{i}\", f\"{i}]]~~\"]\n",
    "\n",
    "emb_layer = EmbeddingModule(\n",
    "    word_counter=word_counter, word_dim=300, specials=specials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataloader for training set\n",
    "from emmental.data import EmmentalDataLoader\n",
    "from fonduer.learning.dataset import FonduerDataset\n",
    "import numpy as np\n",
    "\n",
    "# Filter out noise samples\n",
    "diffs = train_marginals.max(axis=1) - train_marginals.min(axis=1)\n",
    "train_idxs = np.where(diffs > 1e-6)[0]\n",
    "\n",
    "train_dataloader = EmmentalDataLoader(\n",
    "    task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "    dataset=FonduerDataset(\n",
    "        ATTRIBUTE,\n",
    "        train_cands[0],\n",
    "        F_train[0],\n",
    "        emb_layer.word2id,\n",
    "        train_marginals,\n",
    "        train_idxs,\n",
    "    ),\n",
    "    split=\"train\",\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emmental.model import EmmentalModel\n",
    "from fonduer.learning.task import create_task\n",
    "from emmental.learner import EmmentalLearner\n",
    "\n",
    "tasks = create_task(\n",
    "    ATTRIBUTE, 2, F_train[0].shape[1], 2, emb_layer, model=\"LSTM\" # \"LogisticRegression\"\n",
    ")\n",
    "\n",
    "model = EmmentalModel(name=f\"{ATTRIBUTE}_task\")\n",
    "\n",
    "for task in tasks:\n",
    "    model.add_task(task)\n",
    "\n",
    "emmental_learner = EmmentalLearner()\n",
    "emmental_learner.learn(model, [train_dataloader])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on the Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataloader for test data\n",
    "test_dataloader = EmmentalDataLoader(\n",
    "    task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "    dataset=FonduerDataset(\n",
    "        ATTRIBUTE, test_cands[0], F_test[0], emb_layer.word2id, 2\n",
    "    ),\n",
    "    split=\"test\",\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from electricity_utils import entity_level_f1 \n",
    "\n",
    "test_preds = model.predict(test_dataloader, return_preds=True)\n",
    "positive = np.where(np.array(test_preds[\"probs\"][ATTRIBUTE])[:, TRUE] > 0.6)\n",
    "true_pred = [test_cands[0][_] for _ in positive[0]]\n",
    "%time (TP, FP, FN) = entity_level_f1(true_pred, gold_file, ATTRIBUTE, test_docs, stations_mapping_dict=stations_mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Error Analysis & Iterative KBC \n",
    "\n",
    "- Analyise the false positive (FP) and false negative (FN) candidates\n",
    "- Use the visualization tool to better understand which labeling functions might be responsible\n",
    "- Test the labeling functions on this candidates to verify they work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group predictions by keys (stations)\n",
    "result = pandas.DataFrame(true_pred).groupby(\"station\").groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from electricity_utils import entity_to_candidates\n",
    "\n",
    "# Get a list of candidates that match the FN[10] entity\n",
    "fp_cands = entity_to_candidates(FP[240], test_cands[0])\n",
    "\n",
    "\n",
    "# Display a candidate\n",
    "print(fp_cands[0])\n",
    "print(LF_other_station_table(fp_cands[0]))\n",
    "print(f\"Number of FP: {len(FP)}\")\n",
    "vis.display_candidates([fp_cands[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of candidates that match the FN[10] entity\n",
    "tp_cands = entity_to_candidates(TP[40], test_cands[0])\n",
    "\n",
    "\n",
    "# Display a candidate\n",
    "print(f\"Number of TP: {len(TP)}\")\n",
    "print(tp_cands[0])\n",
    "vis.display_candidates([tp_cands[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of candidates that match the FN[10] entity\n",
    "fn_cands = entity_to_candidates(FN[2], test_cands[0])\n",
    "\n",
    "\n",
    "# Display a candidate\n",
    "print(f\"Number of FN: {len(FN)}\")\n",
    "print(fn_cands)\n",
    "vis.display_candidates([fn_cands[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
