{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract electricity prices from VENRON data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we use `Fonduer` to extract relations from the `VENRON` dataset.  \n",
    "This code is a modified version of their original hardware [tutorial](https://github.com/HazyResearch/fonduer-tutorials/tree/master/hardware).  \n",
    "The `Fonduer` pipeline (as outlined in the [paper](https://arxiv.org/abs/1703.05028)), and the iterative KBC process:\n",
    "\n",
    "1. KBC Initialization\n",
    "2. Candidate Generation and Multimodal Featurization\n",
    "3. Probabilistic Relation Classification\n",
    "4. Error Analysis and Iterative KBC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we import the relevant libraries and connect to the local database.  \n",
    "Follow the README instructions to setup the connection to the postgres DB correctly.\n",
    "\n",
    "If the database has existing candidates with generated features, the will not be overriden.  \n",
    "To re-run the entire pipeline including initialization drop the database first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dropdb -h postgres -h postgres --if-exists elec_price_vol\n",
    "! createdb -h postgres -h postgres elec_price_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PARALLEL = 8 # 4  # assuming a quad-core machine\n",
    "ATTRIBUTE = \"elec_price_vol\"\n",
    "DB_USERNAME = 'user'\n",
    "DB_PASSWORD = 'venron'\n",
    "conn_string = f'postgresql://{DB_USERNAME}:{DB_PASSWORD}@postgres:5432/{ATTRIBUTE}'\n",
    "\n",
    "dataset = 'gold' # 'full'    \n",
    "docs_path = f'data/{dataset}/html/'\n",
    "pdf_path = 'data/pdf/'\n",
    "gold_file = 'data/electricity_gold.csv'\n",
    "max_docs = 10 # 114\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "We first initialize a `Meta` object, which manages the connection to the database automatically, and enables us to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fonduer import Meta, init_logging\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "init_logging(log_dir=\"logs\", level=logging.INFO) # DEBUG LOGGING\n",
    "\n",
    "session = Meta.init(conn_string).Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
    "from fonduer.parser.models import Document, Sentence\n",
    "from fonduer.parser import Parser\n",
    "\n",
    "has_documents = session.query(Document).count() > 0\n",
    "\n",
    "corpus_parser = Parser(session, structural=True, lingual=True, visual=True, pdf_path=pdf_path)\n",
    "\n",
    "if (not has_documents): \n",
    "    doc_preprocessor = HTMLDocPreprocessor(docs_path, max_docs=max_docs)\n",
    "    %time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)\n",
    "    \n",
    "print(f\"Documents: {session.query(Document).count()}\")\n",
    "print(f\"Sentences: {session.query(Sentence).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLP library for vector similarities\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dividing the Corpus into Test and Train\n",
    "\n",
    "We'll split the documents 80/10/10 into train/dev/test splits. Note that here we do this in a non-random order to preserve the consistency and we reference the splits by 0/1/2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs][0:5])\n",
    "print(f\"Number of documents split: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Mention Extraction, Candidate Extraction Multimodal Featurization\n",
    "\n",
    "Given the unified data model from Phase 1, `Fonduer` extracts relation\n",
    "candidates based on user-provided **matchers** and **throttlers**. Then,\n",
    "`Fonduer` leverages the multimodality information captured in the unified data\n",
    "model to provide multimodal features for each candidate.\n",
    "\n",
    "## 2.1 Mention Extraction & Candidate Generation\n",
    "\n",
    "1. Define mention classes\n",
    "2. Use matcher functions to define the format of potential mentions\n",
    "3. Define Mentionspaces (Ngrams)\n",
    "4. Run Mention extraction (all possible ngrams in the document, API [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#fonduer.candidates.MentionExtractor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import MentionExtractor \n",
    "from fonduer.candidates.models import Mention\n",
    "from my_subclasses import mention_classes, mention_spaces, matchers\n",
    "hasMentions = session.query(Mention).count() > 0\n",
    "\n",
    "if (not hasMentions):\n",
    "    # 4.) Mention extraction\n",
    "    mention_extractor = MentionExtractor(\n",
    "        session, mention_classes, mention_spaces, matchers\n",
    "    )\n",
    "    docs = session.query(Document).order_by(Document.name).all()\n",
    "    mention_extractor.apply(docs, parallelism=PARALLEL)\n",
    "\n",
    "\n",
    "mentions = session.query(Mention).all()\n",
    "print(f\"Total Mentions: {len(mentions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer_utils import prune_duplicate_mentions\n",
    "\n",
    "Station = mention_classes[0]\n",
    "# Performance increase (reduce quadratic candidates combination by deleting duplicate mentions)\n",
    "mentions = prune_duplicate_mentions(session, mentions, Station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Test if at least one station mention is for meadmktplace types\n",
    "list([x for x in mentions if x.document.name.upper() == \"11_NP 15 PAGES\" and isinstance(x, Station)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Candidate Extraction\n",
    "\n",
    "1. Define Candidate Class\n",
    "2. Define trottlers to reduce the number of possible candidates\n",
    "3. Extract candidates (View the API for the CandidateExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/candidates.html#fonduer.candidates.MentionExtractor).)\n",
    "\n",
    "In the last part we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train/dev/test as splits 0/1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from my_subclasses import candidate_classes, throttlers\n",
    "from fonduer.candidates import CandidateExtractor\n",
    "from fonduer.utils.visualizer import Visualizer\n",
    "\n",
    "\n",
    "# 1.) Define Candidate class\n",
    "StationPrice = candidate_classes[0]\n",
    "has_candidates = session.query(StationPrice).filter(StationPrice.split == 0).count() > 0\n",
    "\n",
    "# 2.) Candidate extraction\n",
    "# NOTE: Without nested_relations flag DocumentMentions and FigureMentions are filtered out. \n",
    "#       Otherwise they would require a rewrite of the featurizers, due to preprocessing we duplicate the img-url and doc-name\n",
    "candidate_extractor = CandidateExtractor(session, [StationPrice], throttlers=throttlers) # , nested_relations=True)\n",
    "\n",
    "for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "    if (not has_candidates):\n",
    "        candidate_extractor.apply(docs, split=i, parallelism=PARALLEL)\n",
    "    print(f\"Number of Candidates in split={i}: {session.query(StationPrice).filter(StationPrice.split == i).count()}\")\n",
    "\n",
    "train_cands = candidate_extractor.get_candidates(split = 0)\n",
    "dev_cands = candidate_extractor.get_candidates(split = 1)\n",
    "test_cands = candidate_extractor.get_candidates(split = 2)\n",
    "cands = [train_cands, dev_cands, test_cands]\n",
    "\n",
    "# 3.) Visualize some candidate for error analysis\n",
    "# pprint(train_cands[0][0])\n",
    "# vis = Visualizer(pdf_path)\n",
    "\n",
    "# Display a candidate\n",
    "# vis.display_candidates([train_cands[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Multimodal Featurization\n",
    "Unlike dealing with plain unstructured text, `Fonduer` deals with richly formatted data, and consequently featurizes each candidate with a baseline library of multimodal features. \n",
    "\n",
    "### Featurize with `Fonduer`'s optimized Postgres Featurizer\n",
    "We now annotate the candidates in our training, dev, and test sets with features. The `Featurizer` provided by `Fonduer` allows this to be done in parallel to improve performance.\n",
    "\n",
    "View the API provided by the `Featurizer` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/features.html#fonduer.features.Featurizer).\n",
    "\n",
    "At the end of this phase, `Fonduer` has generated the set of candidates and the feature matrix. Note that Phase 1 and 2 are relatively static and typically are only executed once during the KBC process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fonduer.features import Featurizer\n",
    "from fonduer.features.models import Feature\n",
    "from fonduer.features.feature_extractors import FeatureExtractor\n",
    "\n",
    "featurizer = Featurizer(\n",
    "    session, \n",
    "    [StationPrice], \n",
    "    feature_extractors=FeatureExtractor([\"textual\", \"structural\", \"tabular\", \"visual\"])\n",
    ")\n",
    "has_features = session.query(Feature).count() > 0\n",
    "\n",
    "if (not has_features):\n",
    "    # Training set\n",
    "    %time featurizer.apply(split=0, train=True, parallelism=PARALLEL)\n",
    "    %time F_train = featurizer.get_feature_matrices(train_cands)\n",
    "    print(F_train[0].shape)\n",
    "\n",
    "    # Dev set\n",
    "    %time featurizer.apply(split=1, parallelism=PARALLEL)\n",
    "    %time F_dev = featurizer.get_feature_matrices(dev_cands)\n",
    "    print(F_dev[0].shape)\n",
    "\n",
    "    # Test set\n",
    "    %time featurizer.apply(split=2, parallelism=PARALLEL)\n",
    "    %time F_test = featurizer.get_feature_matrices(test_cands)\n",
    "    print(F_test[0].shape)\n",
    "else:\n",
    "    %time F_train = featurizer.get_feature_matrices(train_cands)\n",
    "    %time F_dev = featurizer.get_feature_matrices(dev_cands)\n",
    "    %time F_test = featurizer.get_feature_matrices(test_cands)\n",
    "    \n",
    "F = [F_train, F_dev, F_test]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 3: Probabilistic Relation Classification\n",
    "In this phase, `Fonduer` applies user-defined **labeling functions**, which express various heuristics, patterns, and [weak supervision](http://hazyresearch.github.io/snorkel/blog/weak_supervision.html) strategies to label our data, to each of the candidates to create a label matrix that is used by our data programming engine.\n",
    "\n",
    "1. Load Gold Data\n",
    "\n",
    "--- \n",
    "\n",
    "Iterate the following steps\n",
    "\n",
    "2. Create labeling functions\n",
    "3. Apply labeling functions and measure accuracy of each LF (based on gold data).\n",
    "4. Build a generative model by combining the labeling functions\n",
    "5. Iterate on labeling function based on the models score\n",
    "\n",
    "---\n",
    "\n",
    "6. Finally build a descriminative model and test on the test set\n",
    "\n",
    "### 3.1) Loading Gold LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fonduer.supervision.models import GoldLabel\n",
    "from electricity_utils import get_gold_func\n",
    "from fonduer.supervision import Labeler\n",
    "from my_subclasses import stations_mapping_dict\n",
    "\n",
    "# 1.) Load the gold data\n",
    "gold = get_gold_func(gold_file, attribute=ATTRIBUTE, stations_mapping_dict=stations_mapping_dict)\n",
    "docs = corpus_parser.get_documents()\n",
    "labeler = Labeler(session, [StationPrice])\n",
    "%time labeler.apply(docs=docs, lfs=[[gold]], table=GoldLabel, train=True, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2) Creating Labeling Functions\n",
    "\n",
    "We have 3 states that we can return from a LF: `ABSTAIN`, `FALSE` or `TRUE`.\n",
    "\n",
    "A library of data model utilities\n",
    "which can be used to write labeling functions are outline in [Read the\n",
    "Docs](http://fonduer.readthedocs.io/en/stable/user/data_model_utils.html). \n",
    "\n",
    "### 3.3) Applying the Labeling Functions\n",
    "\n",
    "Next, we need to actually run the LFs over all of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database. Note that this will delete any existing `Labels` and `LabelKeys` for this candidate set.\n",
    "\n",
    "View the API provided by the `Labeler` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/supervision.html#fonduer.supervision.Labeler).\n",
    "\n",
    "We can also view statistics about the resulting label matrix.\n",
    "* **Coverage** is the fraction of candidates that the labeling function emits a non-zero label for.\n",
    "* **Overlap** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a non-zero label for.\n",
    "* **Conflict** is the fraction candidates that the labeling function emits a non-zero label for and that another labeling function emits a conflicting non-zero label for.\n",
    "\n",
    "In addition, because we have already loaded the gold labels, we can view the emperical accuracy of these labeling functions when compared to our gold labels using the `analysis` module of [Snorkel](https://github.com/snorkel-team/snorkel)\n",
    "\n",
    "### 3.4) Build Generative Model\n",
    "\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other. To do so, we use [Snorkel](https://github.com/snorkel-team/snorkel)'s single-task label model.\n",
    "\n",
    "We then print out the marginal probabilities for each training candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import *\n",
    "from electricity_utils import eval_LFs\n",
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "from fonduer_utils import get_applied_lfs, get_neighbor_cell_ngrams_own, _min_range_diff, min_row_diff, min_col_diff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "def run_labeling_functions():\n",
    "    ABSTAIN = -1\n",
    "    FALSE = 0\n",
    "    TRUE = 1\n",
    "\n",
    "    @labeling_function()\n",
    "    def LF_other_station_table(c):\n",
    "        station_span = c.station.context.get_span().lower()\n",
    "        neighbour_cells = get_neighbor_cell_ngrams_own(c.price, dist=100, directions=True, n_max = 4, absolute = True)\n",
    "        up_cells = [x for x in neighbour_cells if len(x) > 1 and x[1] == 'DOWN' and x[0] in stations_list]\n",
    "        # No station name in upper cells\n",
    "        if (len(up_cells) == 0):\n",
    "            return ABSTAIN\n",
    "        # Check if the next upper aligned station-span corresponds to the candidate span (or equivalents)\n",
    "        closest_header = up_cells[len(up_cells)-1]\n",
    "        return TRUE if closest_header[0] in stations_mapping_dict[station_span] else FALSE\n",
    "\n",
    "    @labeling_function()\n",
    "    def LF_station_non_meta_tag(c):\n",
    "        html_tags = get_ancestor_tag_names(c.station)\n",
    "        return FALSE if ('head' in html_tags and 'title' in html_tags) else ABSTAIN\n",
    "\n",
    "    # Basic constraint for the price LFs to be true -> no wrong station (increase accuracy)\n",
    "    def base(c):\n",
    "        return (\n",
    "            LF_station_non_meta_tag(c) != 0 and \n",
    "            LF_other_station_table(c) != 0 and \n",
    "            LF_off_peak_head(c) != 0 and\n",
    "            LF_purchases(c)\n",
    "        )\n",
    "\n",
    "    # 2.) Create labeling functions \n",
    "    @labeling_function()\n",
    "    def LF_on_peak_head(c):\n",
    "        return TRUE if 'on peak' in get_aligned_ngrams(c.price, n_min=2, n_max=2)  and base(c) else ABSTAIN\n",
    "\n",
    "    @labeling_function()\n",
    "    def LF_off_peak_head(c):\n",
    "        return FALSE if 'off peak' in get_aligned_ngrams(c.price, n_min=2, n_max=2) else ABSTAIN\n",
    "\n",
    "    @labeling_function()\n",
    "    def LF_price_range(c):\n",
    "        price = float(c.price.context.get_span())\n",
    "        return TRUE if price > 0 and price < 1000 and base(c) else FALSE\n",
    "\n",
    "    @labeling_function()\n",
    "    def LF_price_head(c):\n",
    "        return TRUE if 'price' in get_aligned_ngrams(c.price) and base(c) else ABSTAIN\n",
    "\n",
    "    @labeling_function()\n",
    "    def LF_firm_head(c):\n",
    "        return TRUE if 'firm' in get_aligned_ngrams(c.price)and base(c) else ABSTAIN\n",
    "\n",
    "    @labeling_function()\n",
    "    def LF_dollar_to_left(c):\n",
    "        return TRUE if '$' in get_left_ngrams(c.price, window=2) and base(c) else ABSTAIN\n",
    "\n",
    "    @labeling_function()\n",
    "    def LF_purchases(c):\n",
    "        return FALSE if 'purchases' in get_aligned_ngrams(c.price, n_min=1) else ABSTAIN\n",
    "\n",
    "    station_price_lfs = [\n",
    "        LF_other_station_table,\n",
    "        LF_station_non_meta_tag,\n",
    "\n",
    "        # indicator\n",
    "        LF_price_range,\n",
    "\n",
    "        # negative indicators\n",
    "        LF_off_peak_head,\n",
    "        LF_purchases,\n",
    "\n",
    "        # positive indicators\n",
    "        LF_on_peak_head,    \n",
    "        LF_price_head,\n",
    "        LF_firm_head,\n",
    "        LF_dollar_to_left,\n",
    "    ]\n",
    "\n",
    "    # 3.) Apply the LFs on the training set\n",
    "    labeler = Labeler(session, [StationPrice])\n",
    "    labeler.apply(split=0, lfs=[station_price_lfs], train=True, clear=True, parallelism=PARALLEL)\n",
    "    L_train = labeler.get_label_matrices(train_cands)\n",
    "\n",
    "    # Check that LFs are all applied (avoid crash)\n",
    "    applied_lfs = L_train[0].shape[1]\n",
    "    has_non_applied = applied_lfs != len(station_price_lfs)\n",
    "    print(f\"Labeling functions on train_cands not ABSTAIN: {applied_lfs} (/{len(station_price_lfs)})\")\n",
    "\n",
    "    if (has_non_applied):\n",
    "        applied_lfs = get_applied_lfs(session)\n",
    "        non_applied_lfs = [l.name for l in station_price_lfs if l.name not in applied_lfs]\n",
    "        print(f\"Labling functions {non_applied_lfs} are not applied.\")\n",
    "        station_price_lfs = [l for l in station_price_lfs if l.name in applied_lfs]\n",
    "\n",
    "    # 4.) Evaluate their accuracy\n",
    "    L_gold_train = labeler.get_gold_labels(train_cands, annotator='gold')\n",
    "    # Sort LFs for LFAnalysis because LFAnalysis does not sort LFs,\n",
    "    # while columns of L_train are sorted alphabetically already.\n",
    "    sorted_lfs = sorted(station_price_lfs, key=lambda lf: lf.name)\n",
    "    LFAnalysis(L=L_train[0], lfs=sorted_lfs).lf_summary(Y=L_gold_train[0].reshape(-1))\n",
    "\n",
    "    # 5.) Build generative model\n",
    "    gen_model = LabelModel(cardinality=2)\n",
    "    gen_model.fit(L_train[0], n_epochs=500, log_freq=100)\n",
    "\n",
    "    train_marginals_lfs = gen_model.predict_proba(L_train[0])\n",
    "    plt.hist(train_marginals_lfs[:, TRUE], bins=20)\n",
    "    plt.show()\n",
    "\n",
    "    # Apply on dev-set\n",
    "    labeler.apply(split=1, lfs=[station_price_lfs], clear=True, parallelism=PARALLEL)\n",
    "    L_dev = labeler.get_label_matrices(dev_cands)\n",
    "\n",
    "    L_gold_dev = labeler.get_gold_labels(dev_cands, annotator='gold')\n",
    "    LFAnalysis(L=L_dev[0], lfs=sorted_lfs).lf_summary(Y=L_gold_dev[0].reshape(-1))\n",
    "    return (gen_model, train_marginals_lfs)\n",
    "\n",
    "if (dataset == 'full'):\n",
    "    (gen_model, train_marginals_lfs) = run_labeling_functions()\n",
    "    eval_LFs(train_marginals_lfs, train_cands, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query for analysis\n",
    "# labels = session.query(Label).all()\n",
    "# gold_labels = session.query(GoldLabel).all()\n",
    "# gold_labels_map = { gold_label.candidate_id: gold_label for gold_label in gold_labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fonduer.candidates.models import Candidate\n",
    "\n",
    "# DB_FALSE = FALSE +1\n",
    "# DB_ABSTAIN = ABSTAIN +1\n",
    "# DB_TRUE = TRUE +1\n",
    "\n",
    "# def get_incorrect_instances(lf):\n",
    "#     def is_wrong_label(label):\n",
    "#         if (lf.name not in label.keys):\n",
    "#             return False # Abstain\n",
    "#         assigned_label = label.values[label.keys.index(lf.name)]\n",
    "#         gold_label = gold_labels_map[label.candidate_id] # [x for x in gold_labels if x.candidate_id == label.candidate_id][0]\n",
    "#         return gold_label.values[0] != assigned_label\n",
    "#     return [x.candidate for x in labels if is_wrong_label(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lf = station_price_lfs[5]\n",
    "# wrong_cands = get_incorrect_instances(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(f\"Labeling Function: {lf.name} has wrongly labelled the candidate(1/{len(wrong_cands)}):\")\n",
    "# if (len(wrong_cands) > 0):\n",
    "#     wrong_cand = wrong_cands[100]\n",
    "#     pprint(wrong_cand)\n",
    "#     pprint('LF is True' if lf(wrong_cand) == 1 else 'LF is False')\n",
    "#     vis = Visualizer(pdf_path)\n",
    "\n",
    "#     # Display a candidate\n",
    "#     vis.display_candidates([wrong_cand])\n",
    "# else:\n",
    "#     print(\"There are no wrong candidates for this labeling function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Discriminative Model \n",
    "\n",
    "Fonduer uses the machine learning framework [Emmental](https://github.com/SenWu/emmental) to support all model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emmental\n",
    "import numpy as np\n",
    "\n",
    "from emmental.modules.embedding_module import EmbeddingModule\n",
    "from emmental.data import EmmentalDataLoader\n",
    "from emmental.model import EmmentalModel\n",
    "from emmental.learner import EmmentalLearner\n",
    "from fonduer.learning.utils import collect_word_counter\n",
    "from fonduer.learning.dataset import FonduerDataset\n",
    "from fonduer.learning.task import create_task\n",
    "\n",
    "ABSTAIN = -1\n",
    "FALSE = 0\n",
    "TRUE = 1\n",
    "\n",
    "def train_model(cands, F, train_marginals, model_type=\"LogisticRegression\"):\n",
    "    # Extract candidates and features\n",
    "    train_cands = cands[0]\n",
    "    F_train = F[0]\n",
    "    \n",
    "    # 1.) Setup training config\n",
    "    config = {\n",
    "        \"meta_config\": {\"verbose\": True},\n",
    "        \"model_config\": {\"model_path\": None, \"device\": 0, \"dataparallel\": False},\n",
    "        \"learner_config\": {\n",
    "            \"n_epochs\": 50,\n",
    "            \"optimizer_config\": {\"lr\": 0.001, \"l2\": 0.0},\n",
    "            \"task_scheduler\": \"round_robin\",\n",
    "        },\n",
    "        \"logging_config\": {\n",
    "            \"evaluation_freq\": 1,\n",
    "            \"counter_unit\": \"epoch\",\n",
    "            \"checkpointing\": False,\n",
    "            \"checkpointer_config\": {\n",
    "                \"checkpoint_metric\": {f\"{ATTRIBUTE}/{ATTRIBUTE}/train/loss\": \"min\"},\n",
    "                \"checkpoint_freq\": 1,\n",
    "                \"checkpoint_runway\": 2,\n",
    "                \"clear_intermediate_checkpoints\": True,\n",
    "                \"clear_all_checkpoints\": True,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    emmental.init(Meta.log_path)\n",
    "    emmental.Meta.update_config(config=config)\n",
    "    \n",
    "    # 2.) Collect word counter from training data\n",
    "    word_counter = collect_word_counter(train_cands)\n",
    "    \n",
    "    # 3.) Generate word embedding module for LSTM model\n",
    "    # (in Logistic Regression, we generate it since Fonduer dataset requires word2id dict)\n",
    "    # Geneate special tokens\n",
    "    arity = 2\n",
    "    specials = []\n",
    "    for i in range(arity):\n",
    "        specials += [f\"~~[[{i}\", f\"{i}]]~~\"]\n",
    "\n",
    "    emb_layer = EmbeddingModule(\n",
    "        word_counter=word_counter, word_dim=300, specials=specials\n",
    "    )\n",
    "    \n",
    "    # 4.) Generate dataloader for training set\n",
    "    # Filter out noise samples\n",
    "    diffs = train_marginals.max(axis=1) - train_marginals.min(axis=1)\n",
    "    train_idxs = np.where(diffs > 1e-6)[0]\n",
    "\n",
    "    train_dataloader = EmmentalDataLoader(\n",
    "        task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "        dataset=FonduerDataset(\n",
    "            ATTRIBUTE,\n",
    "            train_cands[0],\n",
    "            F_train[0],\n",
    "            emb_layer.word2id,\n",
    "            train_marginals,\n",
    "            train_idxs,\n",
    "        ),\n",
    "        split=\"train\",\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    # 5.) Training \n",
    "    tasks = create_task(\n",
    "        ATTRIBUTE, 2, F_train[0].shape[1], 2, emb_layer, model=model_type # \"LSTM\" \n",
    "    )\n",
    "\n",
    "    model = EmmentalModel(name=f\"{ATTRIBUTE}_task\")\n",
    "\n",
    "    for task in tasks:\n",
    "        model.add_task(task)\n",
    "\n",
    "    emmental_learner = EmmentalLearner()\n",
    "    emmental_learner.learn(model, [train_dataloader])\n",
    "    \n",
    "    return (model, emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from electricity_utils import entity_level_f1\n",
    "from fonduer_utils import schema_match_filter\n",
    "\n",
    "price_col_keywords = [\"price\", \"weighted avg.\"]  \n",
    "DEBUG = False\n",
    "\n",
    "def eval_model(model, emb_layer, cands, F, schema_filter=False):\n",
    "    # Extract candidates and features \n",
    "    train_cands = cands[0]\n",
    "    dev_cands = cands[1]\n",
    "    test_cands = cands[2] \n",
    "    F_train = F[0]\n",
    "    F_dev = F[1]\n",
    "    F_test = F[2]\n",
    "    \n",
    "    # apply schema filter\n",
    "    def apply(cands):\n",
    "        return schema_match_filter(\n",
    "            cands, \n",
    "            \"station\", \n",
    "            \"price\", \n",
    "            price_col_keywords, \n",
    "            stations_mapping_dict, \n",
    "            0.05,\n",
    "            DEBUG,\n",
    "        )  \n",
    "    \n",
    "    # Generate dataloader for test data\n",
    "    test_dataloader = EmmentalDataLoader(\n",
    "        task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "        dataset=FonduerDataset(\n",
    "            ATTRIBUTE, test_cands[0], F_test[0], emb_layer.word2id, 2\n",
    "        ),\n",
    "        split=\"test\",\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    test_preds = model.predict(test_dataloader, return_preds=True)\n",
    "    positive = np.where(np.array(test_preds[\"probs\"][ATTRIBUTE])[:, TRUE] > 0.6)\n",
    "    true_pred = [test_cands[0][_] for _ in positive[0]]\n",
    "    true_pred = apply(true_pred) if schema_filter else true_pred        \n",
    "    test_results = entity_level_f1(true_pred, gold_file, ATTRIBUTE, test_docs, stations_mapping_dict=stations_mapping_dict)\n",
    "\n",
    "    # Run on dev and train set for validation\n",
    "    # We run the predictions also on our training and dev set, to validate that everything seems to work smoothly\n",
    "    \n",
    "    # Generate dataloader for dev data\n",
    "    dev_dataloader = EmmentalDataLoader(\n",
    "        task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "        dataset=FonduerDataset(\n",
    "            ATTRIBUTE, dev_cands[0], F_dev[0], emb_layer.word2id, 2\n",
    "        ),\n",
    "        split=\"test\",\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    dev_preds = model.predict(dev_dataloader, return_preds=True)\n",
    "    positive_dev = np.where(np.array(dev_preds[\"probs\"][ATTRIBUTE])[:, TRUE] > 0.6)\n",
    "    true_dev_pred = [dev_cands[0][_] for _ in positive_dev[0]]\n",
    "    true_dev_pred = apply(true_dev_pred) if schema_filter else true_dev_pred        \n",
    "    dev_results = entity_level_f1(true_dev_pred, gold_file, ATTRIBUTE, dev_docs, stations_mapping_dict=stations_mapping_dict)\n",
    "\n",
    "    # Generate dataloader for train data\n",
    "    train_dataloader = EmmentalDataLoader(\n",
    "        task_to_label_dict={ATTRIBUTE: \"labels\"},\n",
    "        dataset=FonduerDataset(\n",
    "            ATTRIBUTE, train_cands[0], F_train[0], emb_layer.word2id, 2\n",
    "        ),\n",
    "        split=\"test\",\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    train_preds = model.predict(train_dataloader, return_preds=True)\n",
    "    positive_train = np.where(np.array(train_preds[\"probs\"][ATTRIBUTE])[:, TRUE] > 0.6)\n",
    "    true_train_pred = [train_cands[0][_] for _ in positive_train[0]]\n",
    "    true_train_pred = apply(true_train_pred) if schema_filter else true_train_pred        \n",
    "    train_results = entity_level_f1(true_train_pred, gold_file, ATTRIBUTE, train_docs, stations_mapping_dict=stations_mapping_dict)\n",
    " \n",
    "    return [train_results, dev_results, test_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on the Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on gold labels or labeling functions (gold/full data set)\n",
    "train_marginals_gold = np.array([[0,1] if gold(x) else [1,0] for x in train_cands[0]])\n",
    "train_marginals = train_marginals_gold if dataset == 'gold' else train_marginals_lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from electricity_utils import summarize_results\n",
    "\n",
    "# Build model and evaluate for Logistic Regression\n",
    "(lr_model, lr_emb_layer) = train_model(cands, F, train_marginals, \"LogisticRegression\" )\n",
    "\n",
    "print(\"Evaluate Logistic Regression method\")\n",
    "lr_results = eval_model(lr_model, lr_emb_layer, cands, F)\n",
    "(prec_total, rec_total, f1_total) = summarize_results(lr_results)\n",
    "print(f\"TOTAL DOCS PAIRWISE (LogisticRegression): Precision={prec_total}, Recall={rec_total}, F1={f1_total}\")\n",
    "\n",
    "print(\"Evaluate Logistic Regression method with schema matching\")\n",
    "lr_results = eval_model(lr_model, lr_emb_layer, cands, F, True)\n",
    "(prec_total, rec_total, f1_total) = summarize_results(lr_results)\n",
    "print(f\"TOTAL DOCS PAIRWISE (LogisticRegression): Precision={prec_total}, Recall={rec_total}, F1={f1_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model and evaluate for LSTM\n",
    "(lstm_model, lstm_emb_layer) = train_model(cands, F, train_marginals, \"LSTM\" )\n",
    "\n",
    "print(\"Evaluate LSTM method\")\n",
    "lstm_results = eval_model(lstm_model, lstm_emb_layer, cands, F)\n",
    "(prec_total, rec_total, f1_total) = summarize_results(lstm_results)\n",
    "print(f\"TOTAL DOCS PAIRWISE (LSTM): Precision={prec_total}, Recall={rec_total}, F1={f1_total}\")\n",
    "\n",
    "\n",
    "print(\"Evaluate LSTM method with schema matching\")\n",
    "lstm_results = eval_model(lstm_model, lstm_emb_layer, cands, F, True)\n",
    "(prec_total, rec_total, f1_total) = summarize_results(lstm_results)\n",
    "print(f\"TOTAL DOCS PAIRWISE (LSTM): Precision={prec_total}, Recall={rec_total}, F1={f1_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Error Analysis & Iterative KBC \n",
    "\n",
    "- Analyise the false positive (FP) and false negative (FN) candidates\n",
    "- Use the visualization tool to better understand which labeling functions might be responsible\n",
    "- Test the labeling functions on this candidates to verify they work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from electricity_utils import entity_to_candidates\n",
    "\n",
    "# def display_cand(cand_nr):\n",
    "#     # Get a list of candidates that match the FN[10] entity\n",
    "#     fp_cands = entity_to_candidates(FP[cand_nr], test_cands[0])\n",
    "\n",
    "#     # Display a candidate\n",
    "#     fp_cand = fp_cands[0]\n",
    "#     print(fp_cand)\n",
    "#     print(f\"Number of FP: {cand_nr}/{len(FP)}\")\n",
    "#     vis.display_candidates([fp_cand])\n",
    "#     return fp_cand\n",
    "    \n",
    "# maximum = len(FP)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import widgets\n",
    "# from functools import partial\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "# class Counter:\n",
    "#     def __init__(self, initial=0, maximum=0, minimum=0):\n",
    "#         self.value = initial\n",
    "#         self.maximum = maximum\n",
    "#         self.minimum = 0\n",
    "#         self.cand = display_cand(initial)\n",
    "\n",
    "#     def increment(self, amount=1):\n",
    "#         if (self.value+amount > self.maximum):\n",
    "#             return self.value\n",
    "#         self.value += amount\n",
    "#         return self.value\n",
    "    \n",
    "#     def decrement(self, amount=1):\n",
    "#         if (self.value-amount < 0):\n",
    "#             return self.value\n",
    "#         self.value -= amount\n",
    "#         return self.value\n",
    "\n",
    "#     def __iter__(self, sentinal=False):\n",
    "#         return iter(self.increment, sentinal)\n",
    "    \n",
    "# def display_all(cand_nr):\n",
    "#     # Clear previous\n",
    "#     clear_output(wait=True)\n",
    "#     # Redraw\n",
    "#     display(minus)\n",
    "#     display(plus)\n",
    "#     return display_cand(cand_nr)\n",
    "\n",
    "# def btn_inc(counter, w):\n",
    "#     counter.increment()  \n",
    "#     counter.cand = display_all(counter.value)\n",
    "\n",
    "# def btn_dec(counter, w):\n",
    "#     counter.decrement()\n",
    "#     counter.cand = display_all(counter.value)\n",
    "\n",
    "# counter = Counter(40, maximum)\n",
    "# minus = widgets.Button(description='<')\n",
    "# minus.on_click(partial(btn_dec, counter))\n",
    "\n",
    "# plus = widgets.Button(description='>')\n",
    "# plus.on_click(partial(btn_inc, counter))\n",
    "\n",
    "# display(minus)\n",
    "# display(plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of candidates that match the FN[10] entity\n",
    "# tp_cands = entity_to_candidates(TP[40], test_cands[0])\n",
    "\n",
    "\n",
    "# # Display a candidate\n",
    "# print(f\"Number of TP: {len(TP)}\")\n",
    "# print(tp_cands[0])\n",
    "# vis.display_candidates([tp_cands[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of candidates that match the FN[10] entity\n",
    "# fn_cands = entity_to_candidates(FN[2], test_cands[0])\n",
    "\n",
    "\n",
    "# # Display a candidate\n",
    "# print(f\"Number of FN: {len(FN)}\")\n",
    "# print(fn_cands)\n",
    "# vis.display_candidates([fn_cands[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = re.compile(station_rgx, flags=re.I).search(mentions[len(mentions)-7].document.name)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy \n",
    "# from itertools import chain, tee, groupby, product\n",
    "# from fonduer.utils.data_model_utils.tabular import _get_aligned_sentences\n",
    "# from itertools import groupby\n",
    "# import operator\n",
    "\n",
    "# def get_col(m):\n",
    "#     s = m.context.sentence\n",
    "#     if (not s.is_tabular()):\n",
    "#         return -1\n",
    "#     if (s.cell.col_start != s.cell.col_end):\n",
    "#         return -1\n",
    "#     return s.cell.col_start\n",
    "\n",
    "# def get_headers(mentions_col):\n",
    "#     m_sentences = [m.context.sentence for m in mentions_col]\n",
    "#     min_row = min([x.cell.row_start for x in m_sentences])\n",
    "#     s = m_sentences[0]\n",
    "#     aligned = [x.text for x in _get_aligned_sentences(s, axis=1) if x not in m_sentences and x.cell.row_end < min_row]\n",
    "#     # TODO: HEADER cell-annotation condition\n",
    "#     return aligned\n",
    "\n",
    "# def get_sim(mentions_col_it, fid, pos_keyw, id_dict):\n",
    "#     headers = \" , \".join(get_headers(list(mentions_col_it)))\n",
    "#     pos_keyw_vec = nlp(\" , \".join(pos_keyw + id_dict[fid.context.get_span().lower()]))\n",
    "#     headers_vec = nlp(headers)\n",
    "\n",
    "#     # vectorize with word2vec and measure the similarity to positive/negative schema column keywords\n",
    "#     return pos_keyw_vec.similarity(headers_vec)\n",
    "\n",
    "\n",
    "# def schema_match_filter(cands, id_field, filter_field, pos_keyw = [], id_dict = {}, variance=0.05, DEBUG=False):\n",
    "#     filtered_cands = []\n",
    "    \n",
    "#     # group them by document, itertools requires sorting    \n",
    "#     cands.sort(key=lambda c: c.document.name)\n",
    "#     for doc, doc_it in groupby(cands, lambda c: c.document.name):\n",
    "        \n",
    "#         # group them by the candidate id field (e.g. all prices for one station-id)\n",
    "#         doc_cands = list(doc_it)\n",
    "#         doc_cands.sort(key=lambda c: getattr(c, id_field))\n",
    "#         for fid, doc_cand_it in groupby(doc_cands, lambda c: getattr(c, id_field)):\n",
    "        \n",
    "#             it1, it2, it3 = tee(doc_cand_it, 3)\n",
    "#             # group by col\n",
    "#             doc_ms = [getattr(c, filter_field) for c in iter(it1)]\n",
    "#             doc_ms.sort(key=lambda m: get_col(m))\n",
    "#             ms_by_cols = { col:list(it) for col, it in groupby(doc_ms, lambda m: get_col(m)) }\n",
    "\n",
    "#             # ignore non tabular or multi-col/row \n",
    "#             if (-1 in ms_by_cols.keys()):\n",
    "#                 filtered_cands += [c for c in iter(it2) if getattr(c, filter_field) in ms_by_cols[-1]]\n",
    "\n",
    "#             # Compare headers of each column based on semantic similarity (word vectors)\n",
    "#             similarities = { col:get_sim(it, fid, pos_keyw, id_dict) for col, it in ms_by_cols.items() if col != -1 }\n",
    "#             sim_sorted = [(col, sim) for col, sim in sorted(similarities.items(), key=lambda i: i[1], reverse=True)]\n",
    "#             maximum = sim_sorted[0]\n",
    "            \n",
    "#             # If there is a conflict (multiple assigned columns)\n",
    "#             # only take the maximum similarity as true for this candidate match\n",
    "#             if (len(sim_sorted) > 1 and DEBUG):\n",
    "#                 print(\"#####################################\")\n",
    "#                 print(f\"Similarity for {fid.context.get_span()} in doc {doc}\")\n",
    "#                 print(similarities)\n",
    "#                 print(f\"The maximum similarity is for entries in column {maximum}\")\n",
    "#                 print()\n",
    "#                 for col, it in ms_by_cols.items():\n",
    "#                     print(f\"Col {col} with {len(list(it))} entries and headers:\")\n",
    "#                     pprint(get_headers(list(it)))\n",
    "#                 print()\n",
    "             \n",
    "#             # Filter only the k maximal similar column candidates based on variance\n",
    "#             for i in sim_sorted:\n",
    "#                 if (i[1] >= maximum[1]-variance):\n",
    "#                     if (len(sim_sorted) > 1 and DEBUG):\n",
    "#                         print(\"KEEP\", i)\n",
    "#                     filtered_cands += [c for c in iter(it3) if getattr(c, filter_field) in ms_by_cols[i[0]]]\n",
    "            \n",
    "            \n",
    "#           # only max column\n",
    "# #         counts = { col:len(list(it)) for col, it in ms_by_cols.items() if col != -1 }\n",
    "# #         maximum = max(counts.items(), key=operator.itemgetter(1))[0]\n",
    "# #         if (len(counts) > 1):\n",
    "# #             print(\"max and all\", doc, maximum, counts, get_header(ms_by_cols[maximum][0]))\n",
    "# #             pprint(ms_by_cols)\n",
    "# #             print()\n",
    "            \n",
    "#     return filtered_cands\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "# price_pos_keywords = [\"price\", \"firm\", \"on peak\", \"weighted avg.\"]      \n",
    "# result = schema_match_filter(train_cands[0], \"station\", \"price\", price_pos_keywords, stations_mapping_dict)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(result), \"vs\", len(train_cands[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
